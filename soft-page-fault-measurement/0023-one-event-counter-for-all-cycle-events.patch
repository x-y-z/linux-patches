From d554553282480bf4379a89cfa3c4100639dd42f4 Mon Sep 17 00:00:00 2001
From: Zi Yan <zi.yan@cs.rutgers.edu>
Date: Tue, 4 Nov 2014 16:03:49 -0500
Subject: [PATCH 23/33] one event counter for all cycle events

---
 arch/x86/mm/fault.c | 121 +++++++++++++++++++++++++++++++++++++---------------
 1 file changed, 87 insertions(+), 34 deletions(-)

diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 3d84b42..d35da8c 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -1040,6 +1040,13 @@ static struct perf_event_attr do_page_fault_attr = {
     .disabled     = 1
 };
 
+static struct perf_event_attr in_do_page_fault_attr = {
+    .type         = PERF_TYPE_HARDWARE,
+    .config       = PERF_COUNT_HW_CPU_CYCLES,
+    .size         = sizeof(struct perf_event_attr),
+    .pinned       = 1,
+    .disabled     = 1
+};
 static struct perf_event_attr vma_attr = {
     .type         = PERF_TYPE_HARDWARE,
     .config       = PERF_COUNT_HW_CPU_CYCLES,
@@ -1074,11 +1081,23 @@ __do_page_fault(struct pt_regs *regs, unsigned long error_code,
 	int fault;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 
-	struct perf_event *vma_cycles, *handle_mm_cycles;
-	unsigned long vma_start, vma_total = 0;
+	/*struct perf_event *vma_cycles, *handle_mm_cycles;*/
+	struct perf_event *do_page_fault_event = NULL;
+	unsigned long event_start, event_end, event_total;
+    unsigned long pre_vma_cycles, vma_cycles, vma_to_handle_mm_cycles,
+                  handle_mm_cycles, post_handle_mm_cycles;
     int vma_incache = 0;
     u64 enabled, running;
-    unsigned long handle_mm_start, handle_mm_total = 0;
+    /*unsigned long handle_mm_start, handle_mm_total = 0;*/
+
+    do_page_fault_event =
+        perf_event_create_kernel_counter(&in_do_page_fault_attr,
+                                         -1, tsk, NULL, NULL);
+    if (!IS_ERR(do_page_fault_event)) {
+        perf_event_enable(do_page_fault_event);
+        event_start = perf_event_read_value(do_page_fault_event,
+                                            &enabled, &running);
+    }
 
 	tsk = current;
 	mm = tsk->mm;
@@ -1217,28 +1236,40 @@ retry:
 		might_sleep();
 	}
 
-	vma_cycles = perf_event_create_kernel_counter(&vma_attr, -1, tsk, NULL, NULL);
+	/*vma_cycles = perf_event_create_kernel_counter(&vma_attr, -1, tsk, NULL, NULL);*/
 
     if (vmacache_find(mm, address)) {
         vma_incache = 1;
     }
-	if (!IS_ERR(vma_cycles)) {
-	    perf_event_enable(vma_cycles);
-	    vma_start = perf_event_read_value(vma_cycles, &enabled, &running);
-
+	/*if (!IS_ERR(vma_cycles)) {*/
+		/*perf_event_enable(vma_cycles);*/
+		/*vma_start = perf_event_read_value(vma_cycles, &enabled, &running);*/
+
+    if (!IS_ERR(do_page_fault_event)) {
+        event_end = perf_event_read_value(do_page_fault_event,
+                                            &enabled, &running);
+        pre_vma_cycles = event_end - event_start;
+        event_start = event_end;
+    }
 
-	    vma = find_vma(mm, address);
+	vma = find_vma(mm, address);
 
-	    vma_total = perf_event_read_value(vma_cycles, &enabled, &running) -
-                    vma_start;
-	    perf_event_disable(vma_cycles);
-        perf_event_release_kernel(vma_cycles);
-        vma_cycles = NULL;
-	}
-    else {
-        printk("find vma, Error code: %ld", PTR_ERR(vma_cycles));
-	    vma = find_vma(mm, address);
+    if (!IS_ERR(do_page_fault_event)) {
+        event_end = perf_event_read_value(do_page_fault_event,
+                                            &enabled, &running);
+        vma_cycles = event_end - event_start;
+        event_start = event_end;
     }
+		/*vma_total = perf_event_read_value(vma_cycles, &enabled, &running) -*/
+                    /*vma_start;*/
+		/*perf_event_disable(vma_cycles);*/
+        /*perf_event_release_kernel(vma_cycles);*/
+        /*vma_cycles = NULL;*/
+	/*}*/
+    /*else {*/
+        /*printk("find vma, Error code: %ld", PTR_ERR(vma_cycles));*/
+		/*vma = find_vma(mm, address);*/
+    /*}*/
 
 
 	if (unlikely(!vma)) {
@@ -1283,12 +1314,18 @@ good_area:
 		/*return;*/
 	}
 
-	handle_mm_cycles = perf_event_create_kernel_counter(&handle_mm_attr, -1, tsk, NULL, NULL);
+	/*handle_mm_cycles = perf_event_create_kernel_counter(&handle_mm_attr, -1, tsk, NULL, NULL);*/
 
-	if (!IS_ERR(handle_mm_cycles)) {
-	    perf_event_enable(handle_mm_cycles);
-	    handle_mm_start = perf_event_read_value(handle_mm_cycles, &enabled, &running);
+	/*if (!IS_ERR(handle_mm_cycles)) {*/
+		/*perf_event_enable(handle_mm_cycles);*/
+		/*handle_mm_start = perf_event_read_value(handle_mm_cycles, &enabled, &running);*/
 
+    if (!IS_ERR(do_page_fault_event)) {
+        event_end = perf_event_read_value(do_page_fault_event,
+                                            &enabled, &running);
+        vma_to_handle_mm_cycles = event_end - event_start;
+        event_start = event_end;
+    }
 	/*
 	 * If for any reason at all we couldn't handle the fault,
 	 * make sure we exit gracefully rather than endlessly redo
@@ -1297,16 +1334,22 @@ good_area:
 	 */
 	fault = handle_mm_fault(mm, vma, address, flags);
 
-	    handle_mm_total = perf_event_read_value(handle_mm_cycles, &enabled, &running) -
-                    handle_mm_start;
-	    perf_event_disable(handle_mm_cycles);
-        perf_event_release_kernel(handle_mm_cycles);
-        handle_mm_cycles = NULL;
-	}
-    else {
-        printk("Handle mm fault, Error code: %ld", PTR_ERR(vma_cycles));
-	    fault = handle_mm_fault(mm, vma, address, flags);
+    if (!IS_ERR(do_page_fault_event)) {
+        event_end = perf_event_read_value(do_page_fault_event,
+                                            &enabled, &running);
+        handle_mm_cycles = event_end - event_start;
+        event_start = event_end;
     }
+		/*handle_mm_total = perf_event_read_value(handle_mm_cycles, &enabled, &running) -*/
+                    /*handle_mm_start;*/
+		/*perf_event_disable(handle_mm_cycles);*/
+        /*perf_event_release_kernel(handle_mm_cycles);*/
+        /*handle_mm_cycles = NULL;*/
+	/*}*/
+    /*else {*/
+        /*printk("Handle mm fault, Error code: %ld", PTR_ERR(vma_cycles));*/
+		/*fault = handle_mm_fault(mm, vma, address, flags);*/
+    /*}*/
 	/*
 	 * If we need to retry but a fatal signal is pending, handle the
 	 * signal first. We do not need to release the mmap_sem because it
@@ -1322,6 +1365,11 @@ good_area:
 		/*return;*/
 	}
 
+    if (!IS_ERR(do_page_fault_event)) {
+        event_end = perf_event_read_value(do_page_fault_event,
+                                            &enabled, &running);
+        post_handle_mm_cycles = event_end - event_start;
+    }
 	/*
 	 * Major/minor page fault accounting is only done on the
 	 * initial attempt. If we go through a retry, it is extremely
@@ -1337,9 +1385,9 @@ good_area:
 			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,
 				      regs, address);
 
-            if (vma_total != 0 && handle_mm_total != 0) {
-		        tsk->find_vma_cycles += vma_total;
-                tsk->handle_mm_fault_cycles += handle_mm_total;
+            if (vma_cycles != 0 && handle_mm_cycles != 0) {
+		        tsk->find_vma_cycles += vma_cycles;
+                tsk->handle_mm_fault_cycles += handle_mm_cycles;
 			    tsk->counted_min_flt++;
                 *is_soft_page_fault = 1;
             }
@@ -1367,6 +1415,11 @@ good_area:
 	up_read(&mm->mmap_sem);
 
 return_area:
+    if (!IS_ERR(do_page_fault_event)) {
+        perf_event_disable(do_page_fault_event);
+        perf_event_release_kernel(do_page_fault_event);
+        do_page_fault_event = NULL;
+    }
     return;
 }
 NOKPROBE_SYMBOL(__do_page_fault);
-- 
2.1.4

